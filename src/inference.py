import torch
import argparse
import os,glob
import librosa
import soundfile as sf
import numpy as np
from utils.hparams import HParam
import torch.multiprocessing as mp
from torch.multiprocessing import Pool, Process, set_start_method
from models.GPV import GPV
from models.DGD import DGD


parser = argparse.ArgumentParser()
parser.add_argument('-i','--dir_input',type=str,required=True)
parser.add_argument('-o','--dir_output',type=str,required=True)
parser.add_argument('-c','--config',type=str,required=True)
parser.add_argument('-d','--device',type=str,default='cuda:0')
parser.add_argument('-n','--num_process',type=int,default=8)
parser.add_argument('-p','--chkpt',type=str,required=True)
parser.add_argument('-t','--threshold',type=float,default=0.5)
parser.add_argument('-u','--unit',type=int,default=-1)

args = parser.parse_args()

hp = HParam(args.config)
print('NOTE::Loading configuration :: ' + args.config)

print('inference : {}'.format(args.dir_input))
list_data = glob.glob(os.path.join(args.dir_input,'*.wav'))
print('length : ' + str(len(list_data)))

if len(list_data) == 0:
    raise Exception('Zero targets....')

# Params
device = args.device
torch.cuda.set_device(device)
unit = args.unit

n_mels = hp.model.n_mels
sr = hp.audio.samplerate
n_fft = hp.audio.frame
n_shift = hp.audio.shift

num_epochs = 1
batch_size = 1

channel_in = 1
if 'd' in hp.model.input : 
    channel_in +=1
if 'dd' in hp.model.input : 
    channel_in +=1

model = None
if hp.model.type == "GPV":
    model = GPV(hp,channel_in=channel_in,inputdim=hp.model.n_mels,outputdim=hp.model.label).to(device)
elif hp.model.type == "DGD":
    model = DGD(channel_in=channel_in,dim_input=hp.model.n_mels,dim_output=hp.model.label).to(device)
if model is None :
    raise Exception('No Model specified.')

mel_basis = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)

# Model 
model.load_state_dict(torch.load(args.chkpt,map_location=device))
model.share_memory()
model.eval()

if not args.chkpt == None : 
   print('NOTE::Loading pre-trained model : '+ args.chkpt)
   model.load_state_dict(torch.load(args.chkpt, map_location=device))

threshold = args.threshold
window = [0,
3.07403454443296e-05,
0.000122956752394292,
0.000276635333398266,
0.000491752945027220,
0.000768277191360095,
0.00110616642886250,
0.00150536977265816,
0.00196582710419184,
0.00248746908028317,
0.00307021714356929,
0.00371398353433532,
0.00441867130373081,
0.00518417432836975,
0.00601037732631239,
0.00689715587442620,
0.00784437642712366,
0.00885189633647371,
0.00991956387368396,
0.0110472182519505,
0.0122346896506718,
0.0134817992410230,
0.0147883592128871,
0.0161541728031383,
0.0175790343252736,
0.0190627292003890,
0.0206050339894936,
0.0222057164271593,
0.0238645354564988,
0.0255812412654678,
0.0273555753244860,
0.0291872704253707,
0.0310760507215773,
0.0330216317697404,
0.0350237205725108,
0.0370820156226789,
0.0391962069485813,
0.0413659761607809,
0.0435909965000152,
0.0458709328864056,
0.0482054419699184,
0.0505941721820728,
0.0530367637888852,
0.0555328489450442,
0.0580820517493064,
0.0606839883011065,
0.0633382667583703,
0.0660444873965254,
0.0688022426686980,
0.0716111172670877,
0.0744706881855121,
0.0773805247831091,
0.0803401888491903,
0.0833492346692339,
0.0864072090920075,
0.0895136515978112,
0.0926680943678299,
0.0958700623545856,
0.0991190733534770,
0.102414638075399,
0.105756260220425,
0.109143436552554,
0.112575656975487,
0.116052404609454,
0.119573155869050,
0.123137380542085,
0.126744541869435,
0.130394096625870,
0.134085495201869,
0.137818181686385,
0.141591593950562,
0.145405163732394,
0.149258316722299,
0.153150472649609,
0.157081045369959,
0.161049442953554,
0.165055067774315,
0.169097316599878,
0.173175580682439,
0.177289245850427,
0.181437692601000,
0.185620296193337,
0.189836426742720,
0.194085449315398,
0.198366724024200,
0.202679606124902,
0.207023446113325,
0.211397589823144,
0.215801378524406,
0.220234149022730,
0.224695233759186,
0.229183960910819,
0.233699654491833,
0.238241634455384,
0.242809216795996,
0.247401713652570,
0.252018433411972,
0.256658680813187,
0.261321757052025,
0.266006959886356,
0.270713583741868,
0.275440919818319,
0.280188256196286,
0.284954877944372,
0.289740067226875,
0.294543103411889,
0.299363263179833,
0.304199820632375,
0.309052047401754,
0.313919212760465,
0.318800583731309,
0.323695425197772,
0.328603000014734,
0.333522569119480,
0.338453391642997,
0.343394725021551,
0.348345825108510,
0.353305946286413,
0.358274341579256,
0.363250262764982,
0.368232960488163,
0.373221684372851,
0.378215683135579,
0.383214204698501,
0.388216496302657,
0.393221804621331,
0.398229375873501,
0.403238455937356,
0.408248290463863,
0.413258124990370,
0.418267205054225,
0.423274776306395,
0.428280084625069,
0.433282376229225,
0.438280897792147,
0.443274896554875,
0.448263620439563,
0.453246318162744,
0.458222239348470,
0.463190634641313,
0.468150755819216,
0.473101855906175,
0.478043189284729,
0.482974011808246,
0.487893580912992,
0.492801155729954,
0.497695997196417,
0.502577368167261,
0.507444533525972,
0.512296760295351,
0.517133317747893,
0.521953477515837,
0.526756513700851,
0.531541702983354,
0.536308324731440,
0.541055661109407,
0.545782997185858,
0.550489621041370,
0.555174823875701,
0.559837900114539,
0.564478147515754,
0.569094867275156,
0.573687364131730,
0.578254946472342,
0.582796926435893,
0.587312620016907,
0.591801347168541,
0.596262431904996,
0.600695202403320,
0.605098991104582,
0.609473134814401,
0.613816974802824,
0.618129856903526,
0.622411131612328,
0.626660154185006,
0.630876284734389,
0.635058888326726,
0.639207335077299,
0.643321000245287,
0.647399264327848,
0.651441513153411,
0.655447137974172,
0.659415535557767,
0.663346108278117,
0.667238264205427,
0.671091417195332,
0.674904986977164,
0.678678399241341,
0.682411085725857,
0.686102484301856,
0.689752039058291,
0.693359200385641,
0.696923425058676,
0.700444176318272,
0.703920923952239,
0.707353144375172,
0.710740320707300,
0.714081942852327,
0.717377507574249,
0.720626518573140,
0.723828486559896,
0.726982929329915,
0.730089371835719,
0.733147346258492,
0.736156392078536,
0.739116056144617,
0.742025892742214,
0.744885463660638,
0.747694338259028,
0.750452093531201,
0.753158314169356,
0.755812592626620,
0.758414529178419,
0.760963731982682,
0.763459817138841,
0.765902408745653,
0.768291138957807,
0.770625648041320,
0.772905584427711,
0.775130604766945,
0.777300373979145,
0.779414565305047,
0.781472860355215,
0.783474949157986,
0.785420530206149,
0.787309310502355,
0.789141005603240,
0.790915339662258,
0.792632045471227,
0.794290864500567,
0.795891546938232,
0.797433851727337,
0.798917546602452,
0.800342408124588,
0.801708221714839,
0.803014781686703,
0.804261891277054,
0.805449362675775,
0.806577017054042,
0.807644684591252,
0.808652204500602,
0.809599425053300,
0.810486203601414,
0.811312406599356,
0.812077909623995,
0.812782597393391,
0.813426363784157,
0.814009111847443,
0.814530753823534,
0.814991211155068,
0.815390414498863,
0.815728303736366,
0.816004827982699,
0.816219945594328,
0.816373624175332,
0.816465840582282,
0.816496580927726,
0.816465840582282,
0.816373624175332,
0.816219945594328,
0.816004827982699,
0.815728303736366,
0.815390414498863,
0.814991211155068,
0.814530753823534,
0.814009111847443,
0.813426363784157,
0.812782597393391,
0.812077909623995,
0.811312406599356,
0.810486203601414,
0.809599425053300,
0.808652204500602,
0.807644684591252,
0.806577017054042,
0.805449362675775,
0.804261891277054,
0.803014781686703,
0.801708221714839,
0.800342408124588,
0.798917546602452,
0.797433851727337,
0.795891546938232,
0.794290864500567,
0.792632045471227,
0.790915339662258,
0.789141005603240,
0.787309310502355,
0.785420530206149,
0.783474949157986,
0.781472860355215,
0.779414565305047,
0.777300373979145,
0.775130604766945,
0.772905584427711,
0.770625648041320,
0.768291138957807,
0.765902408745653,
0.763459817138841,
0.760963731982682,
0.758414529178419,
0.755812592626620,
0.753158314169356,
0.750452093531201,
0.747694338259028,
0.744885463660638,
0.742025892742214,
0.739116056144617,
0.736156392078536,
0.733147346258492,
0.730089371835719,
0.726982929329915,
0.723828486559896,
0.720626518573140,
0.717377507574249,
0.714081942852327,
0.710740320707300,
0.707353144375172,
0.703920923952239,
0.700444176318272,
0.696923425058676,
0.693359200385641,
0.689752039058291,
0.686102484301856,
0.682411085725857,
0.678678399241341,
0.674904986977164,
0.671091417195332,
0.667238264205427,
0.663346108278117,
0.659415535557767,
0.655447137974172,
0.651441513153411,
0.647399264327848,
0.643321000245287,
0.639207335077299,
0.635058888326726,
0.630876284734389,
0.626660154185006,
0.622411131612328,
0.618129856903526,
0.613816974802824,
0.609473134814401,
0.605098991104582,
0.600695202403320,
0.596262431904996,
0.591801347168541,
0.587312620016907,
0.582796926435893,
0.578254946472342,
0.573687364131730,
0.569094867275156,
0.564478147515754,
0.559837900114539,
0.555174823875701,
0.550489621041370,
0.545782997185858,
0.541055661109407,
0.536308324731440,
0.531541702983354,
0.526756513700851,
0.521953477515837,
0.517133317747893,
0.512296760295351,
0.507444533525972,
0.502577368167261,
0.497695997196417,
0.492801155729954,
0.487893580912992,
0.482974011808246,
0.478043189284729,
0.473101855906175,
0.468150755819216,
0.463190634641313,
0.458222239348470,
0.453246318162744,
0.448263620439563,
0.443274896554875,
0.438280897792147,
0.433282376229225,
0.428280084625069,
0.423274776306395,
0.418267205054225,
0.413258124990370,
0.408248290463863,
0.403238455937356,
0.398229375873501,
0.393221804621331,
0.388216496302657,
0.383214204698501,
0.378215683135579,
0.373221684372851,
0.368232960488163,
0.363250262764982,
0.358274341579256,
0.353305946286413,
0.348345825108510,
0.343394725021551,
0.338453391642997,
0.333522569119480,
0.328603000014734,
0.323695425197772,
0.318800583731309,
0.313919212760465,
0.309052047401754,
0.304199820632375,
0.299363263179833,
0.294543103411889,
0.289740067226875,
0.284954877944372,
0.280188256196286,
0.275440919818319,
0.270713583741868,
0.266006959886356,
0.261321757052025,
0.256658680813187,
0.252018433411972,
0.247401713652570,
0.242809216795996,
0.238241634455384,
0.233699654491833,
0.229183960910819,
0.224695233759186,
0.220234149022730,
0.215801378524406,
0.211397589823144,
0.207023446113325,
0.202679606124902,
0.198366724024200,
0.194085449315398,
0.189836426742720,
0.185620296193337,
0.181437692601000,
0.177289245850427,
0.173175580682439,
0.169097316599878,
0.165055067774315,
0.161049442953554,
0.157081045369959,
0.153150472649609,
0.149258316722299,
0.145405163732394,
0.141591593950562,
0.137818181686385,
0.134085495201869,
0.130394096625870,
0.126744541869435,
0.123137380542085,
0.119573155869050,
0.116052404609454,
0.112575656975487,
0.109143436552554,
0.105756260220425,
0.102414638075399,
0.0991190733534770,
0.0958700623545856,
0.0926680943678299,
0.0895136515978112,
0.0864072090920075,
0.0833492346692339,
0.0803401888491903,
0.0773805247831091,
0.0744706881855121,
0.0716111172670877,
0.0688022426686980,
0.0660444873965254,
0.0633382667583703,
0.0606839883011065,
0.0580820517493064,
0.0555328489450442,
0.0530367637888852,
0.0505941721820728,
0.0482054419699184,
0.0458709328864056,
0.0435909965000152,
0.0413659761607809,
0.0391962069485813,
0.0370820156226789,
0.0350237205725108,
0.0330216317697404,
0.0310760507215773,
0.0291872704253707,
0.0273555753244860,
0.0255812412654678,
0.0238645354564988,
0.0222057164271593,
0.0206050339894936,
0.0190627292003890,
0.0175790343252736,
0.0161541728031383,
0.0147883592128871,
0.0134817992410230,
0.0122346896506718,
0.0110472182519505,
0.00991956387368396,
0.00885189633647371,
0.00784437642712366,
0.00689715587442620,
0.00601037732631239,
0.00518417432836975,
0.00441867130373081,
0.00371398353433532,
0.00307021714356929,
0.00248746908028317,
0.00196582710419184,
0.00150536977265816,
0.00110616642886250,
0.000768277191360095,
0.000491752945027220,
0.000276635333398266,
0.000122956752394292,
3.07403454443296e-05]


def inference(batch):
    for idx in batch :
        path_target = list_data[idx]
        name_target = path_target.split('/')[-1]

        raw, _ = librosa.load(path_target,sr=sr)

        #raw[:4000]=0.5

        #print(raw[:128])
        spec = librosa.stft(raw,window=window,n_fft=n_fft,hop_length=n_shift, win_length=None,center=True,dtype=np.cdouble)

        #print(np.abs(spec[:,4]))
        mel = np.matmul(mel_basis,np.abs(spec))


        #print(mel[:,4])
        #exit()

        pt = torch.from_numpy(mel)
        pt = pt.float()
        pt = torch.unsqueeze(pt,dim=0)

        shape = pt.shape

        if 'd' in hp.model.input :
            d = torch.zeros(shape)
            # C, dim, T
            d[:,:,:-1] = pt[0,:,1:]-pt[0,:,:-1]
            # channel-wise concat
            pt = torch.cat((pt,d),0)
            if 'dd' in hp.model.input :
                dd = torch.zeros(shape)
                dd[:,:,:-1] = d[0,:,1:]-d[0,:,:-1]
                # channel-wise concat
                pt = torch.cat((pt,dd),0)
        
        #for i in range(10) : 
        #    print("===== m,d,dd" + str(i) + "=====")
        #    print(pt[:,:10,i])
        #exit()

        torch.set_printoptions(profile="full")
        torch.set_printoptions(precision=10)
        pt = pt[:,:,3:]

        #print("=== d 0 ====")
        #print(pt[:,:5,0])        
        #print("=== d 1 ====")
        #print(pt[:,:5,1])        
        #exit()

        pt = torch.unsqueeze(pt,dim=0)
        pt = pt.cuda()


        # pt [ B , C , F, T]

        # offline  
        if unit == -1 : 
            with torch.no_grad():
                output = model(pt)
            output = torch.squeeze(output)
            for idx2 in range(len(output)):
                if output[idx2] < threshold :
                    raw[(idx2-1)*128:idx2*128]=0
        # online 
        else :  
            n_unit = int(np.ceil(pt.shape[3]/unit))
            # process per unit-frame
            with torch.no_grad():
                for i in range(n_unit) : 
                    if i == n_unit-1 :
                        continue
                    else :
                        tmp_pt = pt[:,:,:,i*unit:(i+1)*unit]
                        #tmp_pt=torch.ones(1,3,40,50).to(device)
                        #print(tmp_pt.shape)
                        #print(tmp_pt.dtype)

                    output = model(tmp_pt)
                    output = torch.squeeze(output)
                    #print('====' + str(i) + "=====")
                    #print(output[:3])
                    for idx2 in range(len(output)):
                        if output[idx2] < threshold :
                            raw[(i*unit+idx2-1)*128:(i*unit+idx2)*128]=0
                    #exit()
            

        sf.write(os.path.join(args.dir_output,name_target),raw,16000)

if __name__ == '__main__':
    batch_for_each_process = np.array_split(range(len(list_data)),args.num_process)
    inference(batch_for_each_process[0])
    exit()

    set_start_method('spawn')
    processes = []

    os.makedirs(args.dir_output,exist_ok=True)

    for worker in range(args.num_process):
        p = mp.Process(target=inference, args=(batch_for_each_process[worker][:],) )
        p.start()
        processes.append(p)

    for p in processes:
        p.join()







